## Bitnami RabbitMQ image version
## ref: https://hub.docker.com/r/bitnami/rabbitmq/tags/
##
global:
  ## This registry is the registry what references the rabbitmq and rsyslog images.
  registry: csf-docker-delivered.repo.lab.pl.alcatel-lucent.com
  ## This registry1 is the registry which contains kubectl tools & cbur images.
  registry1: csf-docker-delivered.repo.lab.pl.alcatel-lucent.com
  ## This registry2 is the registry what references the crmq-test image.
  registry2: csf-docker-delivered.repo.lab.pl.alcatel-lucent.com


  podNamePrefix: 

  containerNamePrefix: 

  annotations:

postDeleteJobName: 
postDeleteContainerName: 
postInstallJobName: 
postInstallContainerName: 
postUpgradeJobName: 
postUpgradeContainerName: 
postScaleinJobName: 
postScaleinContainerName: 


image:
  repository: crmq/centos/rabbitmq_3.8.21-1.el7_erlang_24.0.5-1.el7
  tag: 11031
  ## Specify a imagePullPolicy
  ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'
  ## ref: http://kubernetes.io/docs/user-guide/images/#pre-pulling-images
  ##
  pullPolicy: IfNotPresent

  ## set to true if you would like to see extra information on logs
  ## it turns BASH and NAMI debugging in minideb
  ## ref:  https://github.com/bitnami/minideb-extras/#turn-on-bash-debugging
  ## debug: false

  ## Optionally specify an array of imagePullSecrets.
  ## Secrets must be manually created in the namespace.
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
  ##
  # pullSecrets:
  #   - myRegistrKeySecretName

## does your cluster have rbac enabled? assume yes by default

rbac:
  enabled: true
  serviceAccountName:
  serviceAccountNamePostDel :
  serviceAccountNameScale :
  serviceAccountNameAdminForgetnode :
  test:
    enabled: true
    serviceAccountNameHelmTest:
    helmTestSecret:


postDeleteForceClean:
  enabled: false

## does your cluster in pure ipv6 environment?
ipv6Enabled: false

## for mounting the localtime 
localtime: true

custom:
  statefulset:
    annotations:

## to change app.kubernetes.io/part-of: "{{.Values.partOf}}" commonLabel annotation      
partOf: rabbitmq

updateStrategy:
  type: RollingUpdate

securityContext:
  allowPrivilegeEscalation: false
  readOnlyRootFilesystem: true
  runAsUser: 10000
  fsGroup: 10000

istio:
  enabled: false
  # set the version of istio installed in the k8s cluster.
  version: 1.7
  # Whether istio cni is enabled in the environment.
  cni:
    enabled: false
    # MTLS section of configuration.
  mtls:
    #Is strict MTLS enabled in the environment.
    enabled: true
  # Should allow mutual TLS as well as clear text for your deployment.
  # disable to configure mtls in STRICT mode.
  permissive: true

## Whether use istio ingress gateway(Envoy)
istioIngress:
  enabled: false
  # the host used to access the management GUI from istio ingress gateway
  host: "*"
  port: 80
  selector: {istio: ingressgateway}

tmpForceRecreateResources: false

## option clusterDomain to change sts DnsConfig with an other clusterDomain
clusterDomain: cluster.local

## section of specific values for rabbitmq
rabbitmq:
  dynamicConfig:
    enabled: false
    ## Set upgrade to true if you want to run the dynamic config at helm upgrade time.
    ## You should care of not putting already existing rabbitmq ressource otherwise the job fail
    ## for exemple if you already have the foo user declare dont try to declare it again.
    ## If post-upgrade job fail then helm rollaback + change value + helm upgrade again.
    upgrade: false
    maxCommandRetries: 10
    timeout: 300
    # {} cannot be quoted by '
    parameters: |-
      set_policy ha-all "^ha\." '{"ha-mode":"all"}'
    ##set_vm_memory_high_watermark 0.7
    ##add_user "foo" "bar"
    ##add_vhost "/fool"
    ##set_permissions -p '/fool' 'foo' '.' '.' '.*'

  ## RabbitMQ application username
  ## ref: https://github.com/bitnami/bitnami-docker-rabbitmq/blob/master/README.md#creating-a-database-user-on-first-run
  ## this value is mandatory
  username: user
  


  ## RabbitMQ application password
  ## ref: https://github.com/bitnami/bitnami-docker-rabbitmq/blob/master/README.md#creating-a-database-user-on-first-run
  ## this value is mandatory
  password: changeme

  ## Erlang cookie to determine whether different nodes are allowed to communicate with each other
  ## ref: https://github.com/bitnami/bitnami-docker-rabbitmq#environment-variables
  ##
  # erlangCookie:

  ## Node port
  ## ref: https://github.com/bitnami/bitnami-docker-rabbitmq#environment-variables
  ##
  amqpPort: 5672
  ## nodePort: 30010
  ## To expose the amqpPort
  amqpSvc: true

  ## RabbitMQ MQTT configuration
  mqtt:
    enabled: false
    vhost: /
    exchange: amq.topic
    DefaultTcpPort: 1883
    ## tcpNodePort:
    enabledSsl: false
    DefaultSslPort: 8883
    ## sslNodePort:
    DefaultRetainedSave: rabbit_mqtt_retained_msg_store_dets

  ## The node name type in cluster.
  ## When address_type is default, the env RABBITMQ_NODENAME is rabbit@$(POD_NAME).
  ## When address_type is hostname, the env RABBITMQ_NODENAME is rabbit@$(POD_NAME).$(K8S_SERVICE_NAME).$(POD_NAMESPACE).svc.$(k8s_domain).
  ##
  clustering:
    ## The address_type can only be set to default or hostname.
    ## If set to hostname, must set RABBITMQ_USE_LONGNAME="true" in rabbitmq.environment field.
    address_type: default
    k8s_domain: cluster.local   
  ## Node name to cluster with. e.g.: `clusternode@hostname`
  ## ref: https://github.com/bitnami/bitnami-docker-rabbitmq#environment-variables
  ## If rabbitmqClusterNodeName is configured, RABBITMQ_NODENAME is the configured value.
  ##
  # rabbitmqClusterNodeName:

  ## Plugins to enable
  ## You can enable other plugins like:
  ## [rabbitmq_management,rabbitmq_peer_discovery_k8s,rabbitmq_mqtt,rabbitmq_prometheus,...].
  plugins: |-
      [rabbitmq_management,rabbitmq_peer_discovery_k8s,rabbitmq_prometheus].

  ## Configution file content
  configuration: |-
      ## Clustering
      cluster_formation.peer_discovery_backend  = rabbit_peer_discovery_k8s
      cluster_formation.k8s.host = kubernetes.default.svc.cluster.local
      cluster_formation.k8s.address_type = hostname
      cluster_partition_handling = autoheal
      ## queue master locator
      queue_master_locator=min-masters
      ## enable guest user
      loopback_users.guest = false
      ## prometheus plugin (default values when rabbitmq_prometheus plugin is enabled in RabbitMQ 3.8)
      #prometheus.path = /metrics
      #prometheus.tcp.port = 15692

  ## To add some contents in /etc/rabbitmq/advanced.config file
  ## Note: prometheus plugin is no more defined in advancedConfig (since RabbitMQ 3.8)
  advancedConfig: |-
           %%{foo,
           %%  [{bar, [ {path, "/rabbitmq"},
           %%                         {connections_total_enabled, true} ]} ]},
  ## environment file content
  environment: |-
      RABBITMQ_SERVER_ADDITIONAL_ERL_ARGS="+A 128"
      PLUGINS_DIR="/var/lib/rabbitmq/plugins:/usr/lib/rabbitmq/plugins:$RABBITMQ_HOME/plugins"
  environmentIpv6: |-
      RABBITMQ_SERVER_ADDITIONAL_ERL_ARGS="+A 128  -kernel inetrc '/etc/rabbitmq/erl_inetrc'  -proto_dist inet6_tcp"
      RABBITMQ_CTL_ERL_ARGS="-proto_dist inet6_tcp"
      PLUGINS_DIR="/var/lib/rabbitmq/plugins:/usr/lib/rabbitmq/plugins:$RABBITMQ_HOME/plugins"


  ##specific rabbitmq memory configuration memory ( could also be set directly in configuration bloc )
  memory:
    vm_memory_high_watermark_relative:
    vm_memory_high_watermark_absolute:
    vm_memory_high_watermark_paging_ratio:
    disk_free_limit_absolute:
  
  ## This is configuration for RabbitMQ server to support TLS. The configuration to support mqtt tls is also configured here.
  ## Note that here must config the whole content of file, not file path.
  ## The file cacert.pem, cert.pem and key.pem will be mounted to path /etc/rabbitmq.tls.conf/server
  ## If certmanager.use=true then don't fill cacert, cert & key
  tls:
    enabled: false
    cacert: ""
    cert: ""
    key: ""

    ## if you want to use cert file you can give the path ( need to be under /crmq )
    ## exemple
    ##path: 
    ##  cacert: "tls4test/client/cacert.pem"
    ##  cert: "tls4test/client/cert.pem"
    ##  key: "tls4test/client/key.pem"
    path:
      cacert: ""
      cert: ""
      key: ""

    ## if you upload your own certificate directly in the pod please give them path
    uploadPath:
      cacert: ""
      cert: ""
      key: ""

    verify_option: "verify_peer"
    fail_if_no_peer_cert: false
    honor_cipher_order: true
    honor_ecc_order: true
    # Enable options to explicitly set ciphers
    ciphers_options: |-
    ##  ssl_options.ciphers.1 = ECDHE-ECDSA-AES256-GCM-SHA384
    ##  ssl_options.ciphers.2  = ECDHE-RSA-AES256-GCM-SHA384
    test: ""
    ssl_port: 5671
    ## nodePort:

    ## If versions used please fill helm_test_tls_version value
    versions: |-
    ##  ssl_options.versions.1 = tlsv1
    ##  ssl_options.versions.2 = tlsv1.2

    ## If you use tls.version please chose one tls version for tls helm test.
    ##chose one of "ssl.PROTOCOL_TLSv1 ,ssl.PROTOCOL_TLSv1_1, ssl.PROTOCOL_TLSv1_2"
    ##helm_test_tls_version: "ssl.PROTOCOL_TLSv1_2"

    #use_cert_manager replaced by certmanager.used
    certmanager:
      # Require at least BCMT19.12
      used: false
      # 365 days
      duration: "8760h" 
      # 15 days
      renewBefore: "360h"
      keySize: 3072
      issuerName: "ncms-ca-issuer"
      issuerType: "ClusterIssuer"
      issuerGroup: "cert-manager.io"
      # if no commonName provided, the following section is added in secrets.yaml
      ## commonName: {{ printf "%s-server" (include "rabbitmq.fullname" .) | quote }}
      commonName:
      ## rabbitmq.tls.certmanager.ipAddresses can be set either using "helm --set" or by editing the values.yaml
      ## --set "rabbitmq.tls.certmanager.ipAddresses[0]"="127.0.0.1","rabbitmq.tls.certmanager.ipAddresses[1]"="135.1.12.3"
      ##ipAddresses:
      ##  - "127.0.0.1"
      ##  - "135.1.12.3"
      ipAddresses:
      # For "domain" usage, see dnsNames examples hereafter
      domain: svc.cluster.local
      # if no dnsNames provided in values.yaml, then the following section is added in secrets.yaml
      ##dnsNames:
      ##  - "rabbitmq.fullname"
      ##  - "rabbitmq.fullname".{{Release.Namespace}}
      ##  - "rabbitmq.fullname".{{Release.Namespace}}.{{rabbitmq.tls.certmanager.domain}}
      ## rabbitmq.tls.certmanager.dnsNames can be set by following this example (if not set, see above)
      ## --set "rabbitmq.tls.certmanager.dnsNames[0]"="*.mylab.dev","rabbitmq.tls.certmanager.dnsNames[1]"="*.yourlab.dev"
      ## or by following this example:
      ##dnsNames:
      ##  - "*.mylab.dev"
      ##  - "*.yourlab.dev"
  ## This is configuration for management plugin to support TLS.
  ## Note that here must config the whole content of file, not file path.
  ## The file cacert.pem, cert.pem and key.pem will be mounted to path /etc/rabbitmq.tls.conf/management
  ## If certmanager.use=true then don't fill cacert, cert & key
  management:
    enabled: true
    port: 15672
    ## nodePort: 30110
    cacert: ""
    cert: ""
    key: ""
    ## if you want to use cert file you can give the path
    ##path: 
    ##  cacert: "tls4test/client/cacert.pem"
    ##  cert: "tls4test/client/cert.pem"
    ##  key: "tls4test/client/key.pem"
    path:
      cacert: ""
      cert: ""
      key: ""

    ## if you upload your own certificate directly in the pod please give them path
    uploadPath:
      cacert: ""
      cert: ""
      key: ""

    #use_cert_manager replaced by certmanager.used
    certmanager:
      # Require at least BCMT19.12
      used: false
      # 365 days
      duration: "8760h"
      # 15 days
      renewBefore: "360h"
      keySize: 3072
      issuerName: "ncms-ca-issuer"
      issuerType: "ClusterIssuer"
      issuerGroup: "cert-manager.io"
      # if no commonName provided, the following section is added in secrets.yaml
      ## commonName: {{ printf "%s-management" (include "rabbitmq.fullname" .) | quote }}
      commonName:
      ##ipAddresses:
      ##  - "127.0.0.1"
      ##  - "135.1.12.3"
      ipAddresses:
      # For "domain" usage, see dnsNames examples hereafter
      domain: svc.cluster.local
      # if no dnsNames provided in values.yaml, then the following section is added in secrets.yaml
      ## dnsNames:
      ##   - "rabbitmq.management.service"
      ##   - "rabbitmq.management.service".{{Release.Namespace}}
      ##   - "rabbitmq.management.service".{{Release.Namespace}}.{{rabbitmq.management.certmanager.domain}}
      ## rabbitmq.management.certmanager.dnsNames can be set by following this example (if not set, see above)
      ##dnsNames:
      ##  - ""
      ##  - "*"
  prometheus:
    enabled: false
    port: 15692
    ## nodePort: 30111
    tls:
      enabled: false
      cacert: ""
      cert: ""
      key: ""
      ## if you want to use cert file you can give the path
      ##path: 
      ##  cacert: "tls4test/client/cacert.pem"
      ##  cert: "tls4test/client/cert.pem"
      ##  key: "tls4test/client/key.pem"
      path:
        cacert: ""
        cert: ""
        key: ""

      ## if you upload your own certificate directly in the pod please give them path
      uploadPath:
        cacert: ""
        cert: ""
        key: ""

      ## password-if-keyfile-is-encrypted
      password: ""
      certmanager:
        # Require at least BCMT19.12
        used: false
        # 365 days
        duration: "8760h"
        # 15 days
        renewBefore: "360h"
        keySize: 3072
        issuerName: "ncms-ca-issuer"
        issuerType: "ClusterIssuer"
        issuerGroup: "cert-manager.io"
        # if no commonName provided, the following section is added in secrets.yaml
        ## commonName: {{ printf "%s-management" (include "rabbitmq.fullname" .) | quote }}
        commonName:
        ##ipAddresses:
        ##  - "127.0.0.1"
        ##  - "135.1.12.3"
        ipAddresses:
        # For "domain" usage, see dnsNames examples hereafter
        domain: svc.cluster.local
        # if no dnsNames provided in values.yaml, then the following section is added in secrets.yaml
        ## dnsNames:
        ##   - "rabbitmq.management.service"
        ##   - "rabbitmq.management.service".{{Release.Namespace}}
        ##   - "rabbitmq.management.service".{{Release.Namespace}}.{{rabbitmq.management.certmanager.domain}}
        ## rabbitmq.management.certmanager.dnsNames can be set by following this example (if not set, see above)
        ##dnsNames:
        ##  - ""
        ##  - "*"
  uaa:
    enabled: false
    clientId: "rabbit_client"
    serverLocation: "http://localhost:8080/uaa"
  rabbitmqAuthBackendOauth2:
    enabled: false
    ressourceServerId: "rabbitmq"
    keyId: "key-1"
    keyType: "RSA"
    algo: "RS256"
    signing_key: ""
  backuprestore:
    enabled: false
    resources:
      requests:
        memory: 20Mi
        cpu: 100m
      limits:
        memory: 50Mi
        cpu: 1
    ## Refer to: https://confluence.app.alcatel-lucent.com/display/plateng/CBUR+-+Containerized+Backup+and+Recovery+Guide#CBUR-ContainerizedBackupandRecoveryGuide-crd_brpolicyBrPolicy
    backendMode: "local"
    cronJob: "*/10 * * * *"
    brOption: 0
    maxCopy: 5
    agent:
      name: cbura-sidecar
      imageRepo: cbur/cbura
      imageTag: 1.0.3-2919
      imagePullPolicy: IfNotPresent
  # rsyslog: deprecated, use clog instead
  rsyslog:
    enabled: false
    repository: crmq/rsyslog/rsyslog_8.37.0-13.el8
    tag: 20200519124430257-646
    imagePullPolicy: IfNotPresent
    level: debug 
    transport: udp
  clog:
    bcmt:
      enabled: false
    syslog:
      port: 2514
      format: "rfc5424"
      level: debug
      transport: udp
  console:
    enabled: true
    level: info
  
  ## This is the installtion for 3rd party plugins, plugin should be enabled in upper rabbitmq.plugins field.
  ## Note: prometheus is built-in in RabbitMQ 3.8 (so, no more in thirdPartyPlugin)
  ## Configure sample as follow
  ## thirdPartyPlugin:
  ##   - name: <plugin-name>
  ##     path: https://repo.lab.pl.alcatel-lucent.com/csf-generic-delivered/CRMQ/CommunityPlugins
  thirdPartyPlugin:

## This is configuration for client used tls files, such as federation, shovel...
## Note that here must config the whole content of file, not file path.
## Configure sample as follow, the file cacert.pem, cert.pem and key.pem will be mounted to path /etc/rabbitmq.tls.conf/federation.
## tlsClient:
##   - name: "federation"
##     cacert: "..."
##     cert: "..."
##     key: "..."
tlsClient:

## Kubernetes service type
serviceType: ClusterIP
## serviceType: NodePort

persistence:
  ## reserve persistent storage after pod deleted
  reservePvc: false
  reservePvcForScalein: false
  data:
    ## this enables PVC templates that will create one per pod
    enabled: true

    ## rabbitmq data Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    # storageClass: "-"
    accessMode: ReadWriteOnce
    # If you change this value, you might have to adjust `rabbitmq.diskFreeLimit` as well.
    size: 8Gi
  log:
    ## this enables PVC templates that will create one per pod
    enabled: false

    ## rabbitmq data Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    # storageClass: "-"
    accessMode: ReadWriteOnce
    # If you change this value, you might have to adjust `rabbitmq.diskFreeLimit` as well.
    size: 8Gi

kubectlImage:
  repository: tools/kubectl
  tag: v1.23.5-nano-20220401

helmTestImage:
  repository: crmq-test/rabbitmq-test
  tag: 20210825133142108-196

## Configure this value to change delete policy of helm tests. by default it's hook-succeeded,before-hook-creation
helmTestDeletePolicy: ""

## Configure this value to change delete policy of jobs. by default it's hook-succeeded,before-hook-creation
hooks:
  deletePolicy: ""


## Configure resource requests and limits
## ref: http://kubernetes.io/docs/user-guide/compute-resources/
##
resources:
  requests:
    memory: 256Mi
    cpu: 350m
  limits:
    memory: 512Mi
    cpu: 1

jobResources:
  requests:
    cpu: 350m
    memory: 256Mi
  limits:
    cpu: 1
    memory: 512Mi

## Replica count, set to 3 to provide a default available cluster
replicas: 3

## Node labels and tolerations for pod assignment
## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector
## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#taints-and-tolerations-beta-feature
nodeSelector: {}
tolerations: []
affinity: {}

## annotations for rabbitmq pods
podAnnotations: {node.alpha.kubernetes.io/max-pids: "512"}

## annotations for rabbitmq service metadata (else prometheus)
svcAnnotations: {}
## if prometheus plugin is enabled, "prometheus.io/path" must be equal to "rabbitmq.configuration.prometheus.path"
svcPrometheusAnnotations: {prometheus.io/scrape: "true", prometheus.io/path: "/metrics"}

## svcName:

## The following settings are to configure the frequency of the lifeness and readiness probes
livenessProbe:
  enabled: true
  initialDelaySeconds: 120
  timeoutSeconds: 5
  failureThreshold: 6

readinessProbe:
  enabled: true
  initialDelaySeconds: 20
  timeoutSeconds: 6
  periodSeconds: 10

lcm:
   scale_hooks: noupgradehooks
   scale_timeout: 120

ingress:
  enabled: false
  hostName: crmq-gui.paas2.compaas.vlab.pl.alcatel-lucent.com  
  #default path is /
  path:
  tlsSecret: "cm-generated-secret"
  certmanager:
    used: false
    # 365 days
    duration: "8760h"
    # 15 days
    renewBefore: "360h"
    keySize: 3072
    issuerName: "ncms-ca-issuer"
    issuerType: "ClusterIssuer"
    issuerGroup: "cert-manager.io"
##  ingress.certmanager.dnsNames can be set by following this example
##    dnsNames: |
##      - ""
##      - "*"
